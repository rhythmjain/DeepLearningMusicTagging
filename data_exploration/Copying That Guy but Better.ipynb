{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb94593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f5b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992793b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a58fb134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6a3fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf43be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac67a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "566df40f",
   "metadata": {},
   "source": [
    "# Defining Dataseat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "494e5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(torch.utils.data.Dataset):   \n",
    "    def __init__(self, np_file_paths, labels, seq_len=10000):\n",
    "        self.seq_len = seq_len\n",
    "        self.files = np_file_paths\n",
    "        self.padder = torch.zeros(96, seq_len)\n",
    "        self.labels = labels\n",
    "#         self.labels = []\n",
    "#         for i in range(len(self.files)):\n",
    "#             label = np.random.randint(0, 10, size=15)\n",
    "#             label[label > 8] = 0\n",
    "#             label[label >= 1] = 1\n",
    "#             self.labels.append(label)\n",
    "#         for i in range(len(self.files)):\n",
    "#             label = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "#             self.labels.append(label)\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return (self.X[index:index+self.seq_len], self.y[index+self.seq_len-1])\n",
    "    def __getitem__(self, index):\n",
    "        x = np.load(self.files[index])\n",
    "        x = torch.from_numpy(x).float()\n",
    "        x = x[:,:self.seq_len]\n",
    "        x = pad_sequence([x.T, self.padder.T], padding_value=-90, batch_first=True)[0].T\n",
    "#         input,label_ids,label\n",
    "        item = {\"input\": x, \"label_ids\":[index], \"labels\": torch.tensor(self.labels[index])}\n",
    "        return item\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b81fdb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c432a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce9f573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import disable_caching\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b8124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c28a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = ['00/1164200.mp3',\n",
    " '00/12100.mp3',\n",
    " '00/1295900.mp3',\n",
    " '00/985000.mp3',\n",
    " '00/1398500.mp3',\n",
    " '00/1210700.mp3',\n",
    " '00/818600.mp3',\n",
    " '00/1339600.mp3',\n",
    " '00/506100.mp3',\n",
    " '00/390000.mp3',\n",
    " '00/16000.mp3',\n",
    " '00/1052800.mp3',\n",
    " '00/699100.mp3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5968b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_files = ['01/16101.mp3',\n",
    " '01/1052801.mp3',\n",
    " '01/12101.mp3',\n",
    " '01/1121101.mp3',\n",
    " '01/986601.mp3',\n",
    " '01/1125001.mp3',\n",
    " '01/1086601.mp3',\n",
    " '01/1219101.mp3',\n",
    " '01/759301.mp3',\n",
    " '01/1018801.mp3',\n",
    " '01/824301.mp3',\n",
    " '01/1167301.mp3',\n",
    " '01/1380601.mp3',\n",
    " '01/661601.mp3',\n",
    " '01/1398501.mp3',\n",
    " '01/390001.mp3',\n",
    " '01/80501.mp3',\n",
    " '01/1125401.mp3',\n",
    " '01/399201.mp3',\n",
    " '01/1210701.mp3',\n",
    " '01/554901.mp3',\n",
    " '01/292501.mp3',\n",
    " '01/842401.mp3',\n",
    " '01/1157701.mp3',\n",
    " '01/1245901.mp3',\n",
    " '01/1062501.mp3',\n",
    " '01/1189901.mp3',\n",
    " '01/1398801.mp3',\n",
    " '01/1357701.mp3',\n",
    " '01/1164201.mp3',\n",
    " '01/1396501.mp3',\n",
    " '01/1304001.mp3',\n",
    " '01/913701.mp3',\n",
    " '01/718301.mp3',\n",
    " '01/1381001.mp3',\n",
    " '01/1264201.mp3',\n",
    " '01/361701.mp3',\n",
    " '01/1420701.mp3',\n",
    " '01/1406401.mp3',\n",
    " '01/708401.mp3',\n",
    " '01/1009701.mp3',\n",
    " '01/846501.mp3',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b668f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_to_labels(files):\n",
    "    col_names = ['TRACK_ID',\n",
    "     'ARTIST_ID',\n",
    "     'ALBUM_ID',\n",
    "     'PATH',\n",
    "     'DURATION',\n",
    "     'TAGS',\n",
    "     'TAGS2',\n",
    "     'TAGS3',\n",
    "     'TAGS4',\n",
    "     'TAGS5',\n",
    "     'TAGS6',\n",
    "     'TAGS7',\n",
    "     'TAGS8',\n",
    "     'TAGS9']\n",
    "    MOODPATH = \"/mnt/c/Users/aag12/Downloads/autotagging_moodtheme.tsv.txt\"\n",
    "    df = pd.read_csv(MOODPATH, sep='\\t', names=col_names)\n",
    "    df = df[df[\"PATH\"].isin(files)]\n",
    "    inds = {'fast': 0,\n",
    "     'sexy': 1,\n",
    "     'mellow': 2,\n",
    "     'heavy': 3,\n",
    "     'horror': 4,\n",
    "     'travel': 5,\n",
    "     'holiday': 6,\n",
    "     'groovy': 7,\n",
    "     'funny': 8,\n",
    "     'retro': 9,\n",
    "     'hopeful': 10,\n",
    "     'powerful': 11,\n",
    "     'cool': 12,\n",
    "     'nature': 13,\n",
    "     'game': 14}\n",
    "\n",
    "    final_labels = []\n",
    "    for i in range(len(df)):\n",
    "        curr = np.zeros(len(inds))\n",
    "        moods = list(df.iloc[i])[5:]\n",
    "        for theme in moods:\n",
    "            if type(theme) == str and \"mood\" in theme:\n",
    "                check = theme.split(\"---\")[-1]\n",
    "                if check in inds:\n",
    "                    curr[inds[check]] = 1\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        final_labels.append(curr)\n",
    "    return final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07d2d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = files_to_labels(good_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d874d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = files_to_labels(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "545d632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_np_files = [(\"/mnt/c/Users/aag12/Documents/subset_moodtheme/\" + g).replace(\".mp3\", \".npy\") for g in good_files]\n",
    "test_np_files = [(\"/mnt/c/Users/aag12/Documents/subset_moodtheme/\" + g).replace(\".mp3\", \".npy\") for g in test_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd90cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "284ea16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEQ_LEN = 10000\n",
    "SEQ_LEN = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50166183",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MusicDataset(train_np_files, train_labels, seq_len=SEQ_LEN)\n",
    "test_dataset = MusicDataset(test_np_files, test_labels, seq_len=SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31ae25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc8858f9",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af63012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6e6c45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res2DMaxPoolModule(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, pooling=2):\n",
    "        super(Res2DMaxPoolModule, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(input_channels, output_channels, 3, padding=1)\n",
    "        self.bn_1 = nn.BatchNorm2d(output_channels)\n",
    "        self.conv_2 = nn.Conv2d(output_channels, output_channels, 3, padding=1)\n",
    "        self.bn_2 = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mp = nn.MaxPool2d(pooling)\n",
    "\n",
    "        # residual\n",
    "        self.diff = False\n",
    "        if input_channels != output_channels:\n",
    "            self.conv_3 = nn.Conv2d(input_channels, output_channels, 3, padding=1)\n",
    "            self.bn_3 = nn.BatchNorm2d(output_channels)\n",
    "            self.diff = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn_2(self.conv_2(self.relu(self.bn_1(self.conv_1(x)))))\n",
    "        if self.diff:\n",
    "            x = self.bn_3(self.conv_3(x))\n",
    "        out = x + out\n",
    "        out = self.mp(self.relu(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResFrontEnd(nn.Module):\n",
    "    \"\"\"\n",
    "    Evaluation of CNN based Music Tagging.\n",
    "    Won et al., 2020\n",
    "    \n",
    "    Note that, different from the original work, we only stack 3 convolutional layers instead of 7.\n",
    "    After the convolution layers, we flatten the time-frequency representation to be a vector.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conv_ndim, attention_ndim, nfreq, nharmonics=1):\n",
    "        super(ResFrontEnd, self).__init__()\n",
    "        self.input_bn = nn.BatchNorm2d(nharmonics)\n",
    "        self.layer1 = Res2DMaxPoolModule(nharmonics, conv_ndim, pooling=(2, 2))\n",
    "        self.layer2 = Res2DMaxPoolModule(conv_ndim, conv_ndim, pooling=(2, 2))\n",
    "        self.layer3 = Res2DMaxPoolModule(conv_ndim, conv_ndim, pooling=(2, 1))\n",
    "#         fc_ndim = nfreq // 2 // 2 // 2 * conv_ndim\n",
    "#         self.fc = nn.Linear(fc_ndim, attention_ndim)\n",
    "\n",
    "    def forward(self, hcqt):\n",
    "        # batch normalization\n",
    "        out = self.input_bn(hcqt)\n",
    "\n",
    "        # CNN\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "\n",
    "        # permute and channel control\n",
    "#         b, c, f, t = out.shape\n",
    "#         out = out.permute(0, 3, 1, 2)  # batch, time, conv_ndim, freq\n",
    "#         out = out.contiguous().view(b, t, -1)  # batch, time, fc_ndim\n",
    "#         out = self.fc(out)  # batch, time, attention_ndim\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d0be72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "47b613bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTaggingTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_ndim=16,\n",
    "        n_mels=128,\n",
    "        sample_rate=22050,\n",
    "        n_fft=1024,\n",
    "        f_min=0,\n",
    "        f_max=11025,\n",
    "        attention_ndim=256,\n",
    "        attention_nheads=8,\n",
    "        attention_nlayers=4,\n",
    "        attention_max_len=512,\n",
    "        dropout=0.1,\n",
    "        n_seq_cls=1,\n",
    "        n_token_cls=1,\n",
    "    ):\n",
    "        super(MusicTaggingTransformer, self).__init__()\n",
    "        # Input preprocessing\n",
    "#         self.spec = self.spec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "#                                                                      n_fft=n_fft,\n",
    "#                                                                      f_min=f_min,\n",
    "#                                                                      f_max=f_max,\n",
    "#                                                                      n_mels=n_mels,\n",
    "#                                                                      power=2)\n",
    "#         self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        \n",
    "        # Input embedding\n",
    "        self.frontend = ResFrontEnd(conv_ndim, attention_ndim, n_mels)\n",
    "\n",
    "        # Positional embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, attention_max_len + 1, attention_ndim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(attention_ndim))\n",
    "\n",
    "        # transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            attention_ndim,\n",
    "            attention_nlayers,\n",
    "            attention_nheads,\n",
    "            attention_ndim // attention_nheads,\n",
    "            attention_ndim * 4,\n",
    "            dropout,\n",
    "        )\n",
    "        self.to_latent = nn.Identity()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # projection for sequence classification\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(attention_ndim), nn.Linear(attention_ndim, n_seq_cls)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch, time)\n",
    "        Returns:\n",
    "            x (torch.Tensor): (batch, n_seq_cls)\n",
    "        \"\"\"\n",
    "        # Input preprocessing\n",
    "#         x = self.spec(x)\n",
    "#         x = self.amplitude_to_db(x)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        # Input embedding\n",
    "#         print(x.shape)\n",
    "        x = self.frontend(x)\n",
    "\n",
    "        # Positional embedding with a [CLS] token\n",
    "#         cls_token = self.cls_token.repeat(x.shape[0], 1, 1)\n",
    "        print(x.shape)\n",
    "        print(self.pos_embedding.shape)\n",
    "#         print(cls_token.shape)\n",
    "#         x = torch.cat((cls_token, x), dim=1)\n",
    "        x += self.pos_embedding[:, : x.size(1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # transformer\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # projection for sequence classification\n",
    "        x = self.to_latent(x[:, 0])\n",
    "        x = self.mlp_head(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "031f9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
    "\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
    "        mask_value = -torch.finfo(dots.dtype).max\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = F.pad(mask.flatten(1), (1, 0), value=True)\n",
    "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
    "            mask = mask[:, None, :] * mask[:, :, None]\n",
    "            dots.masked_fill_(~mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        Residual(\n",
    "                            PreNorm(\n",
    "                                dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)\n",
    "                            )\n",
    "                        ),\n",
    "                        Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x, mask=mask)\n",
    "            x = ff(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "1048f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BetterMusicTaggingTransformer(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         conv_ndim=16,\n",
    "#         attention_ndim=256,\n",
    "#         attention_nheads=8,\n",
    "#         attention_nlayers=4,\n",
    "#         attention_max_len=512,\n",
    "#         dropout=0.1,\n",
    "#         n_seq_cls=1,\n",
    "#         n_token_cls=1,\n",
    "#     ):\n",
    "#         super(MusicTaggingTransformer, self).__init__()\n",
    "#         # Input preprocessing\n",
    "#         self.convdownsampler = nn.Conv2d(1, 1, kernel_size=5, stride=3,padding=2)\n",
    "\n",
    "#         # Positional embedding\n",
    "#         self.pos_embedding = nn.Parameter(torch.randn(1, attention_max_len + 1, attention_ndim))\n",
    "#         self.cls_token = nn.Parameter(torch.randn(attention_ndim))\n",
    "\n",
    "#         # transformer\n",
    "#         self.transformer = Transformer(\n",
    "#             attention_ndim,\n",
    "#             attention_nlayers,\n",
    "#             attention_nheads,\n",
    "#             attention_ndim // attention_nheads,\n",
    "#             attention_ndim * 4,\n",
    "#             dropout,\n",
    "#         )\n",
    "# #         attention_ndim=256\n",
    "# #         attention_nheads=8\n",
    "# #         attention_nlayers=4\n",
    "# #         attention_max_len=512\n",
    "# #         dropout=0.1\n",
    "# #         trans = nn.Transformer(\n",
    "# #                     d_model=attention_ndim,\n",
    "# #                     nhead=attention_nheads,\n",
    "# #                     num_encoder_layers=attention_nlayers,\n",
    "# #         #             attention_ndim // attention_nheads,\n",
    "# #         #             attention_ndim * 4,\n",
    "# #         #             dropout,\n",
    "# #                 )\n",
    "#         self.to_latent = nn.Identity()\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#         # projection for sequence classification\n",
    "#         self.mlp_head = nn.Sequential(\n",
    "#             nn.LayerNorm(attention_ndim), nn.Linear(attention_ndim, n_seq_cls)\n",
    "#         )\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             x (torch.Tensor): (batch, time)\n",
    "#         Returns:\n",
    "#             x (torch.Tensor): (batch, n_seq_cls)\n",
    "#         \"\"\"\n",
    "#         # Input preprocessing\n",
    "#         x = self.spec(x)\n",
    "#         x = self.amplitude_to_db(x)\n",
    "#         x = x.unsqueeze(1)\n",
    "\n",
    "#         # Input embedding\n",
    "#         x = self.frontend(x)\n",
    "\n",
    "#         # Positional embedding with a [CLS] token\n",
    "#         cls_token = self.cls_token.repeat(x.shape[0], 1, 1)\n",
    "#         x = torch.cat((cls_token, x), dim=1)\n",
    "#         x += self.pos_embedding[:, : x.size(1)]\n",
    "#         x = self.dropout(x)\n",
    "\n",
    "#         # transformer\n",
    "#         x = self.transformer(x)\n",
    "\n",
    "#         # projection for sequence classification\n",
    "#         x = self.to_latent(x[:, 0])\n",
    "#         x = self.mlp_head(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "2209ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MusicTaggingTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f7924261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-21.8261, -33.5147, -30.9810,  ..., -84.2810, -84.0638, -83.7539],\n",
       "         [-24.3652, -16.3998, -22.2266,  ..., -22.0788, -26.8455, -26.1931],\n",
       "         [ -1.6416,   9.6201,   2.6366,  ..., -13.9094, -10.7504,  -8.3305],\n",
       "         ...,\n",
       "         [ -9.7697, -12.6982, -13.0742,  ..., -15.4942, -17.7372, -15.5838],\n",
       "         [-17.2892, -17.1960, -12.8223,  ..., -26.6157, -27.6857, -26.9247],\n",
       "         [-14.8324, -15.5061, -14.3807,  ..., -26.3400, -28.4942, -28.8055]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "33b76297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 40, 256]), torch.Size([1, 96, 1000]))"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo.shape, train_dataset[0]['input'].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "41eb67fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 12, 250])\n",
      "torch.Size([1, 513, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (250) must match the size of tensor b (256) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [223]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [217]\u001b[0m, in \u001b[0;36mMusicTaggingTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#         print(cls_token.shape)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#         x = torch.cat((cls_token, x), dim=1)\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m         x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding[:, : x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     76\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;66;03m# transformer\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (250) must match the size of tensor b (256) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "m(train_dataset[0]['input'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f20e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_ndim=256\n",
    "attention_nheads=8\n",
    "attention_nlayers=4\n",
    "attention_max_len=512\n",
    "dropout=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa738c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = nn.Transformer(\n",
    "            d_model=attention_ndim,\n",
    "            nhead=attention_nheads,\n",
    "            num_encoder_layers=attention_nlayers,\n",
    "#             attention_ndim // attention_nheads,\n",
    "#             attention_ndim * 4,\n",
    "#             dropout,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ed987155",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(1, 1, kernel_size=5, stride=3,padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e73e4bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': tensor([[-75.1938, -62.6215, -69.7594,  ..., -11.2562,  -1.3265,   7.3618],\n",
       "         [-81.3514, -68.9420, -75.0238,  ...,  -1.7271,  -1.1692,  11.6628],\n",
       "         [-85.0144, -77.8768, -85.4316,  ...,   8.5874,  -4.7166,  10.7350],\n",
       "         ...,\n",
       "         [-90.0000, -88.7078, -86.6313,  ..., -30.7656, -32.4680, -26.8488],\n",
       "         [-90.0000, -90.0000, -90.0000,  ..., -38.2618, -40.8892, -37.0017],\n",
       "         [-90.0000, -90.0000, -90.0000,  ..., -54.0475, -58.1458, -55.8609]]),\n",
       " 'label_ids': [0],\n",
       " 'labels': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        dtype=torch.float64)}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f458e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "23997838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16 * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6367ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([Resize((32, 320)), ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c0c7423c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 1000])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['input'].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "42d347be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 334])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = conv(train_dataset[0]['input'].unsqueeze(0))\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a5c3f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "128a8dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 334])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5ff9d21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-21.8261, -33.5147, -30.9810,  ...,  -3.1035,  -3.5209,  -0.4347],\n",
       "         [-44.1574, -72.5931, -73.2856,  ...,   2.4495,   1.3631,   3.0209],\n",
       "         [-49.0809, -78.1875, -81.8709,  ...,   7.3148,   5.7103,   9.7803],\n",
       "         ...,\n",
       "         [-49.0031, -84.2731, -83.1274,  ..., -16.0451, -17.5599, -22.2684],\n",
       "         [-48.6362, -82.5790, -79.1320,  ..., -21.1351, -22.0427, -24.7242],\n",
       "         [-49.4997, -83.3667, -82.4678,  ..., -26.3400, -28.4942, -28.8055]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[:,:,:320]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8ec6e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4cc681be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 256])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rearrange(o[:,:,:320].unsqueeze(0), 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size)\n",
    "rearrange(o[:,:,:320], 'b (h s1) (w s2) -> b (h w) (s1 s2)', s1=patch_size, s2=patch_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d5393349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24 * 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65c57a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab58f80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['labels'].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fd47ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "bo = rearrange(o[:,:,:320], 'b (h s1) (w s2) -> b (h w) (s1 s2)', s1=patch_size, s2=patch_size)\n",
    "bl = train_dataset[0]['labels'].unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0fe2bc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 40, 256]), torch.Size([1, 1, 15]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo.shape, bl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ce479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075293ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d467fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "912b8f66",
   "metadata": {},
   "source": [
    "# DUMB TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e2570fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004eb16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0edba90d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [137]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m v \u001b[38;5;241m=\u001b[39m ViT(\n\u001b[1;32m      2\u001b[0m     image_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m      3\u001b[0m     patch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     emb_dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m preds \u001b[38;5;241m=\u001b[39m v(img)\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "v = ViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 32,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "img = torch.randn(256, 256)\n",
    "img = torch.tensor([[img, img, img]])\n",
    "\n",
    "preds = v(img) # (1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4304a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.stack([img, img, img], dim=0).unsqueeze(0)\n",
    "\n",
    "preds = v(img) # (1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "11fdfb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b0a92d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 40, 256]), torch.Size([1, 1, 15]))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo.shape, bl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3382d566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2bc79c0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "the batch number of src and tgt must be equal",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [128]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/transformer.py:138\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    136\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m!=\u001b[39m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: the batch number of src and tgt must be equal"
     ]
    }
   ],
   "source": [
    "trans(bo, bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75caeeb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d206b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.conv_1 = nn.Conv2d(input_channels, output_channels, 3, padding=1)\n",
    "self.bn_1 = nn.BatchNorm2d(output_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bf2f1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "80e52770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3dfe40ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427acb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77412d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1689e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0b2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py38)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
